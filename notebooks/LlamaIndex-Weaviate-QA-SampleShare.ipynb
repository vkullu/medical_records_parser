{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own your health app!\n",
    "\n",
    "Trying to show a simple PoC where I give it a medical test document and then ask \n",
    "some structured responses from it. \n",
    "\n",
    "For our real app, we will have 10's or 100's of such documents per individual \n",
    "and each document could be 10's to 100's of pages. This is because someone's diagnostic \n",
    "journey e.g. cancer is spread across tests and visits to 100's of institutions before they\n",
    "get refered to a large cancer center oncologist and this oncology team has to make sense of all you have\n",
    "endured {treatment, outcomes, discharge summaries} to give you the right next treatment when \n",
    "you arrive at their doorstep.\n",
    "\n",
    "\n",
    "For now success will be if for a single report I am able to get back all the\n",
    "diagnostic tests done without missing. It tends to miss a few and on repeated nudging since \n",
    "I know the answer, pull out more and more tests missed previously.\n",
    "Also, as a bonus what I would really really like would be I ask a question with a schema \n",
    "and it returns *all* the elements in the same schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to your weaviate instance\n",
    "\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "  embedded_options=EmbeddedOptions()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternative text](../docs/images/PXL_20230726_203549965.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a pathology report, containing 7 some pages of all sorts of blood work essential at a particular cancer journey stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = SimpleDirectoryReader('../data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk up the data posts into nodes \n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "\n",
    "# construct vector store\n",
    "vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"Cancer_patient_bloodwork\", text_key=\"content\")\n",
    "\n",
    "# setting up the storage for the embeddings\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "#\n",
    "# set up the index\n",
    "index = VectorStoreIndex(nodes, storage_context = storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting report into pre determined types. This can also be done via enums in \"class\" code.\n",
    "query_engine = index.as_query_engine()\n",
    "question = \"\"\"\n",
    "What kind of report is this? \n",
    "Your choices are pathology report, lab report, genetic test report, clinical notes or radiology report.\n",
    "\"\"\"\n",
    "report_type = query_engine.query(question)\n",
    "print(report_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting report into pre determined types. This can also be done via enums in \"class\" code.\n",
    "query_engine = index.as_query_engine()\n",
    "question = \"\"\"\n",
    "Please give me back a table of all the analytes measured. The table should have following columns: analyte_measured, result, reference_interval, unit, notes. \n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any analyte.\n",
    "\"\"\"\n",
    "response = query_engine.query(str(report_type)+question)\n",
    "print(response)\n",
    "# It's only doing the first page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did it come to the conclusion that it was a patholiogy report?\n",
    "\n",
    "evidence = response.source_nodes[0].node.text\n",
    "location = response.source_nodes[0].node.metadata\n",
    "evidence, location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more harder queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to configure the retriever so I can get *all* results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it more complex since we need top 'n' results not only top 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please give me back a table of all the analytes measured. The table should have following columns: analyte_measured, result, reference_interval, unit, test_date, page_number, notes. \n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any analyte.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(question)\n",
    "print(response)\n",
    "# Major issues:\n",
    "# It is missing a lot of values, sometimes partially parsing pages\n",
    "# Also, here, I specify k=10, this cannot be pre-configurable and needs to be done every page at a time\n",
    "# Also it is making up wrong page numbers.\n",
    "\n",
    "# Minor issues:\n",
    "# Seems like on this one it is giving NA for entries which are found randomly in text. \n",
    "# Do I need better prompting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
