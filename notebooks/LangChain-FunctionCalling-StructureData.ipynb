{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own your health app!\n",
    "\n",
    "Trying to show a simple PoC where I give it a medical test document and then ask \n",
    "some structured responses from it. \n",
    "\n",
    "For our real app, we will have 10's or 100's of such documents per individual \n",
    "and each document could be 10's to 100's of pages. This is because someone's diagnostic \n",
    "journey e.g. cancer is spread across tests and visits to 100's of institutions before they\n",
    "get refered to a large cancer center oncologist and this oncology team has to make sense of all you have\n",
    "endured {treatment, outcomes, discharge summaries} to give you the right next treatment when \n",
    "you arrive at their doorstep.\n",
    "\n",
    "\n",
    "For now success will be if for a single report I am able to get back all the\n",
    "diagnostic tests done without missing. It tends to miss a few and on repeated nudging since \n",
    "I know the answer, pull out more and more tests missed previously.\n",
    "Also, as a bonus what I would really really like would be I ask a question with a schema \n",
    "and it returns *all* the elements in the same schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# Right now this does not do directory, it takes one file at a time, could this be a limitation later !?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach is doing this with one big gulp, no splitting and using function calls to structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core algorithm which parses the PDF file and structres the output.\n",
    "# the plus of using pedantic models is yo can force what you want the return obect to look like\n",
    "# pedantic then does the validation to make sure!\n",
    "# In the future we can make this configurable!\n",
    "\n",
    "#TODO: Adding a list object into the pedayntic object which itself is an object?\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.openai_functions.extraction import _get_extraction_function\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/medical_records_parser/data/MinnieMouseReport.pdf\")\n",
    "# Above I read the whole thing as ONE large blob, this was possible since the file is only 7 pages!\n",
    "# if the file becomes too large, this is not possible.\n",
    "docs = doc.load()\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "You are an expert medical transcriber. Please give me back a table of all the analytes measured. The table should have following columns: analyte_measured, result, reference_interval, unit, notes. \n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any analyte.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Analyte(BaseModel):\n",
    "    \"\"\"Information about an analyte.\"\"\"\n",
    "    analyte_measured: str\n",
    "    result: str\n",
    "    reference_interval: str\n",
    "    unit: str\n",
    "    notes: str\n",
    "\n",
    "openai_function = _get_extraction_function(Analyte.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "\n",
    "output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "response = chain.invoke({\"doc\": docs[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(response) # Amit/Guy can you please see if the output is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach is doing the same thing as above but one page at a time since sometimes the file might be too large to fit context size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/medical_records_parser/data/MinnieMouseReport.pdf\", mode=\"paged\")\n",
    "# Above is the key difference where it is loading the data as \"paged\" mode.\n",
    "\n",
    "docs = doc.load()\n",
    "\n",
    "query = \"\"\"\n",
    "Please give me back a table of all the analytes measured. The table should have following columns: analyte_measured, result, reference_interval, unit, notes. \n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any analyte.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "# This is because of what _get_extraction_function does\n",
    "output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})\n",
    "\n",
    "extracted_by_function_call = []\n",
    "for response in responses:\n",
    "    extracted_by_function_call.extend(response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(extracted_by_function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the third approach, this is a VERY large document of 130+ pages including mishmash of diffrent kind of reports since it has come from an EMR dump (likely EPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/kaiser/data/Barbara/UCLA Health.pdf\")\n",
    "docs = doc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the entire contents of the document\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need some kind of splitter which closely resembles where records start and end\n",
    "# I am using the default on, which is sub optimal and does a lot of repeats and not so useful summarization too!\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 10000,\n",
    "    chunk_overlap  = 500,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "doc = UnstructuredPDFLoader(\"/Users/vinayak/projects/kaiser/data/Barbara/UCLA Health.pdf\")\n",
    "\n",
    "docs = doc.load_and_split(text_splitter)\n",
    "\n",
    "print(\"Number of splits %d\"%(len(docs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice I also changed the question from parsing a diagnostic report to parsing medical visits. Likely we will have to do a hybrid where we first split the very large document into different pieces and for each piece parse what is relevant (diagnostic report vs visits vs pathology report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "You are an expert medical transcriber and can transcribe electronic health records with great skill.\n",
    "Please give me back a table of all the visits from the patient. Columns to return are:\n",
    "patient_name, date_of_visit, category, provider, institution, brief_summary\n",
    "The category can only be one of the following values: LAB_REPORT, PATHOLOGY, RADIOLOGY, PROCEDURE, DIAGNOSTIC_TEST, ROUTINE_VISIT\n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any visits.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.openai_functions.extraction import _get_extraction_function\n",
    "\n",
    "class Visit(BaseModel):\n",
    "    \"\"\"Information about visit to the medical facility.\"\"\"\n",
    "    patient_name: str\n",
    "    date_of_visit: str\n",
    "    category: str\n",
    "    provider: str\n",
    "    institution: str\n",
    "    brief_summary: str\n",
    "\n",
    "openai_function = _get_extraction_function(Visit.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "# Make subset of docs below (8) so I don't become bankrupt! with openAI bills\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})\n",
    "\n",
    "\n",
    "## The above code aks a question per subset of the data (according to the split which is 10k). This means it will have\n",
    "## answers per split. The response object is list of response, each response hiving a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to flatten the list\n",
    "\n",
    "flattened_list = list()\n",
    "for d in responses:\n",
    "    flattened_list.extend(d)\n",
    "\n",
    "flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(flattened_list)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing cleanup to remove junk, this is because our parser is not yet good enough.\n",
    "\n",
    "df1 = df[(df.patient_name != 'NA')]\n",
    "df1 = df1[(df1.category != 'NA')]\n",
    "\n",
    "df1['date_cleanedup']= pd.to_datetime(df1['date_of_visit'], format='mixed')\n",
    "df1['final_date'] = df1['date_cleanedup'].apply(lambda x: x.strftime('%B %d, %Y'))\n",
    "\n",
    "df1.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "\n",
    "df1.sort_values(by='date_cleanedup').to_csv('/Users/vinayak/projects/df_to_test.tsv', sep=\"\\t\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={'final_date': 'title','institution': 'cardTitle', 'category': 'cardSubtitle', 'brief_summary': 'cardDetailedText' }, inplace=True)\n",
    "cols_needed = ['title', 'cardTitle', 'cardSubtitle', 'cardDetailedText']\n",
    "df1[cols_needed].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The stuff below is working but chunking is per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/kaiser/data/Barbara/UCLA Health.pdf\", mode=\"paged\")\n",
    "\n",
    "docs = doc.load()\n",
    "\n",
    "query = \"\"\"\n",
    "Please give me back a table of all the visits from the patient. Columns to return are:\n",
    "visit_date, visit_reason, visit_department, visit_summary\n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any visits.\n",
    "\"\"\"\n",
    "\n",
    "class Visit(BaseModel):\n",
    "    \"\"\"Information about visit to the medical facility.\"\"\"\n",
    "    visit_date: str\n",
    "    visit_reason: str\n",
    "    visit_department: str\n",
    "    visit_summary: str\n",
    "\n",
    "\n",
    "\n",
    "openai_function = _get_extraction_function(Visit.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "# This is because of what _get_extraction_function does\n",
    "output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
