{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own your health app!\n",
    "\n",
    "Trying to show a simple PoC where I give it a medical test document and then ask \n",
    "some structured responses from it. \n",
    "\n",
    "For our real app, we will have 10's or 100's of such documents per individual \n",
    "and each document could be 10's to 100's of pages. This is because someone's diagnostic \n",
    "journey e.g. cancer is spread across tests and visits to 100's of institutions before they\n",
    "get refered to a large cancer center oncologist and this oncology team has to make sense of all you have\n",
    "endured {treatment, outcomes, discharge summaries} to give you the right next treatment when \n",
    "you arrive at their doorstep.\n",
    "\n",
    "\n",
    "For now success will be if for a single report I am able to get back all the\n",
    "diagnostic tests done without missing. It tends to miss a few and on repeated nudging since \n",
    "I know the answer, pull out more and more tests missed previously.\n",
    "Also, as a bonus what I would really really like would be I ask a question with a schema \n",
    "and it returns *all* the elements in the same schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# Right now this does not do directory, it takes one file at a time, could this be a limitation later !?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach is doing this with one big gulp, no splitting and using function calls to structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core algorithm which parses the PDF file and structres the output.\n",
    "# the plus of using pedantic models is yo can force what you want the return obect to look like\n",
    "# pedantic then does the validation to make sure!\n",
    "# In the future we can make this configurable!\n",
    "\n",
    "#TODO: Adding a list object into the pydantic object which itself is an object?\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.openai_functions.extraction import _get_extraction_function\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/medical_records_parser/data/MinnieMouseReport.pdf\")\n",
    "# Above I read the whole thing as ONE large blob, this was possible since the file is only 7 pages!\n",
    "# if the file becomes too large, this is not possible.\n",
    "docs = doc.load()\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "You are an expert medical transcriber. Please give me back a table of all the analytes measured. The table should have following columns: analyte_measured, result, reference_interval, unit, notes. \n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any analyte.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Analyte(BaseModel):\n",
    "    \"\"\"Information about an analyte.\"\"\"\n",
    "    analyte_measured: str\n",
    "    result: str\n",
    "    reference_interval: str\n",
    "    unit: str\n",
    "    notes: str\n",
    "\n",
    "openai_function = _get_extraction_function(Analyte.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\") #this approach will work as far as document size is small.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "\n",
    "output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "response = chain.invoke({\"doc\": docs[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(response) # Amit/Guy can you please see if the output is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach is doing the same thing as above but one page at a time since sometimes the file might be too large to fit context size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/medical_records_parser/data/MinnieMouseReport.pdf\", mode=\"paged\")\n",
    "# Above is the key difference where it is loading the data as \"paged\" mode.\n",
    "\n",
    "docs = doc.load()\n",
    "\n",
    "query = \"\"\"\n",
    "Please give me back a table of all the analytes measured. The table should have following columns: analyte_measured, result, reference_interval, unit, notes. \n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any analyte.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "# This is because of what _get_extraction_function does\n",
    "output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})\n",
    "\n",
    "extracted_by_function_call = []\n",
    "for response in responses:\n",
    "    extracted_by_function_call.extend(response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(extracted_by_function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the third approach, this is a VERY large document of 130+ pages including mishmash of diffrent kind of reports since it has come from an EMR dump (likely EPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/kaiser/data/Barbara/UCLA Health.pdf\")\n",
    "docs = doc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the entire contents of the document\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need some kind of splitter which closely resembles where records start and end\n",
    "# I am using the default on, which is sub optimal and does a lot of repeats and not so useful summarization too!\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 10000,\n",
    "    chunk_overlap  = 500,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "doc = UnstructuredPDFLoader(\"/Users/vinayak/projects/kaiser/data/Barbara/UCLA Health.pdf\")\n",
    "\n",
    "docs = doc.load_and_split(text_splitter)\n",
    "\n",
    "print(\"Number of splits %d\"%(len(docs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice I also changed the question from parsing a diagnostic report to parsing medical visits. Likely we will have to do a hybrid where we first split the very large document into different pieces and for each piece parse what is relevant (diagnostic report vs visits vs pathology report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "You are an expert medical transcriber and can transcribe electronic health records with great skill.\n",
    "Please give me back a table of all the visits from the patient. Columns to return are:\n",
    "patient_name, date_of_visit, category, provider, institution, brief_summary\n",
    "The category can only be one of the following values: LAB_REPORT, PATHOLOGY, RADIOLOGY, PROCEDURE, DIAGNOSTIC_TEST, ROUTINE_VISIT\n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any visits.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.openai_functions.extraction import _get_extraction_function\n",
    "\n",
    "class Visit(BaseModel):\n",
    "    \"\"\"Information about visit to the medical facility.\"\"\"\n",
    "    patient_name: str\n",
    "    date_of_visit: str\n",
    "    category: str\n",
    "    provider: str\n",
    "    institution: str\n",
    "    brief_summary: str\n",
    "\n",
    "openai_function = _get_extraction_function(Visit.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "# Make subset of docs below (8) so I don't become bankrupt! with openAI bills\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})\n",
    "\n",
    "\n",
    "## The above code aks a question per subset of the data (according to the split which is 10k). This means it will have\n",
    "## answers per split. The response object is list of response, each response hiving a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to flatten the list\n",
    "\n",
    "flattened_list = list()\n",
    "for d in responses:\n",
    "    flattened_list.extend(d)\n",
    "\n",
    "flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(flattened_list)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing cleanup to remove junk, this is because our parser is not yet good enough.\n",
    "\n",
    "df1 = df[(df.patient_name != 'NA')]\n",
    "df1 = df1[(df1.category != 'NA')]\n",
    "\n",
    "df1['date_cleanedup']= pd.to_datetime(df1['date_of_visit'], format='mixed')\n",
    "df1['final_date'] = df1['date_cleanedup'].apply(lambda x: x.strftime('%B %d, %Y'))\n",
    "\n",
    "df1.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "\n",
    "df1.sort_values(by='date_cleanedup').to_csv('/Users/vinayak/projects/df_to_test.tsv', sep=\"\\t\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={'final_date': 'title','institution': 'cardTitle', 'category': 'cardSubtitle', 'brief_summary': 'cardDetailedText' }, inplace=True)\n",
    "cols_needed = ['title', 'cardTitle', 'cardSubtitle', 'cardDetailedText']\n",
    "df1[cols_needed].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now trying with GPT4 instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "You are an expert medical transcriber and can transcribe electronic health records with great skill.\n",
    "Please give me back a table of all the visits from the patient. Columns to return are:\n",
    "patient_name, date_of_visit, category, provider, institution, brief_summary\n",
    "The category can only be one of the following values: LAB_REPORT, PATHOLOGY, RADIOLOGY, PROCEDURE, DIAGNOSTIC_TEST, ROUTINE_VISIT\n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any visits.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.openai_functions.extraction import _get_extraction_function\n",
    "\n",
    "class Visit(BaseModel):\n",
    "    \"\"\"Information about visit to the medical facility.\"\"\"\n",
    "    patient_name: str\n",
    "    date_of_visit: str\n",
    "    category: str\n",
    "    provider: str\n",
    "    institution: str\n",
    "    brief_summary: str\n",
    "\n",
    "openai_function = _get_extraction_function(Visit.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "# Make subset of docs below (8) so I don't become bankrupt! with openAI bills\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})\n",
    "\n",
    "\n",
    "## The above code aks a question per subset of the data (according to the split which is 10k). This means it will have\n",
    "## answers per split. The response object is list of response, each response hiving a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to flatten the list\n",
    "\n",
    "flattened_list = list()\n",
    "for d in responses:\n",
    "    flattened_list.extend(d)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(flattened_list)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[(df.patient_name != 'NA')]\n",
    "df1 = df1[(df1.category != 'NA')]\n",
    "\n",
    "df1['date_cleanedup']= pd.to_datetime(df1['date_of_visit'], format='mixed')\n",
    "df1['final_date'] = df1['date_cleanedup'].apply(lambda x: x.strftime('%B %d, %Y'))\n",
    "\n",
    "df1.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "\n",
    "df1.sort_values(by='date_cleanedup').to_csv('/Users/vinayak/projects/df_to_test_gpt4.tsv', sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is with Antropic Claudin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "You are an expert medical transcriber and can transcribe electronic health records with great skill.\n",
    "Please give me back a table of all the visits from the patient. Columns to return are:\n",
    "patient_name, date_of_visit, category, provider, institution, brief_summary\n",
    "The category can only be one of the following values: LAB_REPORT, PATHOLOGY, RADIOLOGY, PROCEDURE, DIAGNOSTIC_TEST, ROUTINE_VISIT\n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any visits.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.openai_functions.extraction import _get_extraction_function\n",
    "\n",
    "class Visit(BaseModel):\n",
    "    \"\"\"Information about visit to the medical facility.\"\"\"\n",
    "    patient_name: str\n",
    "    date_of_visit: str\n",
    "    category: str\n",
    "    provider: str\n",
    "    institution: str\n",
    "    brief_summary: str\n",
    "\n",
    "openai_function = _get_extraction_function(Visit.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = AnthropicFunctions(model='claude-2')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "# Make subset of docs below (8) so I don't become bankrupt! with openAI bills\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})\n",
    "\n",
    "\n",
    "## The above code aks a question per subset of the data (according to the split which is 10k). This means it will have\n",
    "## answers per split. The response object is list of response, each response hiving a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The stuff below is working but chunking is per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/kaiser/data/Barbara/UCLA Health.pdf\", mode=\"paged\")\n",
    "\n",
    "docs = doc.load()\n",
    "\n",
    "query = \"\"\"\n",
    "Please give me back a table of all the visits from the patient. Columns to return are:\n",
    "visit_date, visit_reason, visit_department, visit_summary\n",
    "If a patricular column does not exist please say NA.\n",
    "Please double check your work and do not miss any visits.\n",
    "\"\"\"\n",
    "\n",
    "class Visit(BaseModel):\n",
    "    \"\"\"Information about visit to the medical facility.\"\"\"\n",
    "    visit_date: str\n",
    "    visit_reason: str\n",
    "    visit_department: str\n",
    "    visit_summary: str\n",
    "\n",
    "\n",
    "\n",
    "openai_function = _get_extraction_function(Visit.schema())\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query),\n",
    "    (\"user\", \"{doc}\")\n",
    "])\n",
    "\n",
    "# This is because of what _get_extraction_function does\n",
    "output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\n",
    "\n",
    "chain = prompt | model.bind(functions=[openai_function], function_call={\"name\": \"information_extraction\"}) | output_parser\n",
    "\n",
    "\n",
    "responses = chain.batch([{\"doc\": d.page_content} for d in docs], {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport sys\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "\n",
    "def edit_pdf_text(input_pdf_path, output_pdf_path, old_text, new_text):\n",
    "    # Read the existing PDF\n",
    "    with open(input_pdf_path, \"rb\") as file_handle:\n",
    "        pdf = PdfFileReader(file_handle)\n",
    "        content = pdf.getPage(0).extractText()\n",
    "\n",
    "    # Replace the old text with the new text\n",
    "    content = content.replace(old_text, new_text)\n",
    "\n",
    "    # Write the modified content to a new PDF\n",
    "    pdf_writer = PdfFileWriter()\n",
    "    pdf_writer.addPage(pdf.getPage(0))\n",
    "    with open(output_pdf_path, \"wb\") as output_pdf:\n",
    "        pdf_writer.write(output_pdf)\n",
    "        \n",
    "edit_pdf_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2 pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pdfplumber\n",
    "\n",
    "def replace_text_in_pdf(input_pdf_path, output_pdf_path, text_to_find, replacement_text):\n",
    "    with pdfplumber.open(input_pdf_path) as pdf:\n",
    "        pages = pdf.pages\n",
    "        #print(pages)\n",
    "        for i, page in enumerate(pages):\n",
    "            text = page.extract_text()\n",
    "            #print(text)\n",
    "            replaced_text = text.replace(\"Mouse\", \"Vinayak\")\n",
    "            pages[i] = replaced_text\n",
    "\n",
    "    with open(output_pdf_path, 'wb') as output_pdf:\n",
    "        pdf_writer = PyPDF2.PdfWriter()\n",
    "        for page in pages:\n",
    "            print(page)\n",
    "            pdf_writer.add_page(page)\n",
    "        pdf_writer.write('~/Desktop/Vinayak.pdf')\n",
    "        \n",
    "replace_text_in_pdf(input_pdf_path, output_pdf_path, 'Minnie', 'Vinayak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_text_in_pdf(input_pdf_path, output_pdf_path, 'Minnie', 'Vinayak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def change_text(pdf_file, old_text, new_text):\n",
    "  \"\"\"\n",
    "  This function changes the text in a PDF file.\n",
    "\n",
    "  Args:\n",
    "    pdf_file: The path to the PDF file.\n",
    "    old_text: The text to be replaced.\n",
    "    new_text: The new text.\n",
    "\n",
    "  Returns:\n",
    "    None.\n",
    "  \"\"\"\n",
    "\n",
    "  pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "  pdf_writer = PyPDF2.PdfWriter()\n",
    "\n",
    "  for page in pdf_reader.pages:\n",
    "    text = page.extract_text()\n",
    "    text = text.replace(old_text, new_text)\n",
    "    page.(text)\n",
    "\n",
    "  pdf_writer.write(pdf_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  pdf_file = input_pdf_path\n",
    "  old_text = \"This is the old text.\"\n",
    "  new_text = \"This is the new text.\"\n",
    "\n",
    "  change_text(pdf_file, old_text, new_text)\n",
    "\n",
    "  print(\"Text successfully changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install borb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chapter_007/src/snippet_013.py\n",
    "from borb.pdf import Document\n",
    "from borb.pdf import PDF\n",
    "from borb.toolkit import SimpleFindReplace\n",
    "\n",
    "import typing\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # attempt to read a PDF\n",
    "    doc: typing.Optional[Document] = None\n",
    "    with open(input_pdf_path, \"rb\") as pdf_file_handle:\n",
    "        doc = PDF.loads(pdf_file_handle)\n",
    "\n",
    "    # check whether we actually read a PDF\n",
    "    assert doc is not None\n",
    "\n",
    "    # find/replace\n",
    "    doc = SimpleFindReplace.sub(\"Minnie\", \"Vinayak\", doc)\n",
    "\n",
    "    # store\n",
    "    with open(output_pdf_path, \"wb\") as pdf_file_handle:\n",
    "        PDF.dumps(pdf_file_handle, doc)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pdfplumber\n",
    "\n",
    "def replace_text_in_pdf(input_pdf_path, output_pdf_path, text_to_find, replacement_text):\n",
    "    # Lists to hold the text content and their bounding boxes\n",
    "    replacements = []\n",
    "\n",
    "    # Extract text and their bounding boxes using pdfplumber\n",
    "    with pdfplumber.open(input_pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            for word in page.extract_words():\n",
    "                if text_to_find in word['text']:\n",
    "                    replaced_text = word['text'].replace(text_to_find, replacement_text)\n",
    "                    bbox = (word['x0'], word['y0'], word['x1'], word['y1'])\n",
    "                    replacements.append((replaced_text, bbox))\n",
    "\n",
    "    # Open the PDF with PyPDF2\n",
    "    with open(input_pdf_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "        writer = PyPDF2.PdfFileWriter()\n",
    "\n",
    "        for page_num in range(reader.numPages):\n",
    "            page = reader.getPage(page_num)\n",
    "\n",
    "            # Overlay the replacement texts\n",
    "            for text, bbox in replacements:\n",
    "                x0, y0, x1, y1 = bbox\n",
    "                # Adjust the coordinates as needed\n",
    "                page.merge_text(text, x0, y0, size=y1-y0)\n",
    "\n",
    "            writer.addPage(page)\n",
    "\n",
    "        # Write the modified content to the output PDF\n",
    "        with open(output_pdf_path, 'wb') as output_pdf:\n",
    "            writer.write(output_pdf)\n",
    "\n",
    "replace_text_in_pdf(input_pdf_path, '/Users/vinayak/Desktop/Vinaya.pdf', 'Mouse', 'Vinayak')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section is trying to map a file to a type of report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/kaiser/data/barbara_split/ucla_1-6.pdf\")\n",
    "\n",
    "docs = doc.load()\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "You are an expert medical transcriber. Given a document you can clearly distinguish and categorize it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"aggressiveness\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"enum\": [1, 2, 3, 4, 5],\n",
    "            \"description\": \"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        },\n",
    "        \"language\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"spanish\", \"english\", \"french\", \"german\", \"italian\"],\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"language\", \"sentiment\", \"aggressiveness\"],\n",
    "}\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "chain = create_tagging_chain(schema, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marvin import ai_classifier\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "@ai_classifier\n",
    "class ReportClassifer(Enum):\n",
    "    \"\"\"You are an expert clinican and medical notes interpreter. Classify the report based on which part of the healthcare network it came from\"\"\"\n",
    "\n",
    "    DIAGNOSTIC_REPORT = 1\n",
    "    OFFICE_VISIT = 2\n",
    "    SURGERY_VISIT = 3\n",
    "    RADIOLOGY_REPORT = 4\n",
    "    BLOOD_WORK = 5\n",
    "    MEDICATION_LIST = 6\n",
    "\n",
    "ReportClassifer(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "doc = UnstructuredPDFLoader(file_path=\"/Users/vinayak/projects/kaiser/data/barbara_split/ucla_104-107.pdf\")\n",
    "\n",
    "docs = doc.load()\n",
    "\n",
    "ReportClassifer(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trying summarization for the document\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "\n",
    "\n",
    "from langchain.document_loaders.image import UnstructuredImageLoader\n",
    "\n",
    "loader = UnstructuredImageLoader(\"/Users/vinayak/projects/kaiser/data/tcga_scanned_image/TCGA1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the summary of the document?\"\n",
    "index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Please give me a two part answer to my question. \n",
    "First, starting with Answer: is the answer to the question and second paragraph starting with Citation: the exact lines from the document used to give the answer\n",
    "What is the summary of this oncology report?\n",
    "\"\"\"\n",
    "index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What kind of cancer does the patient have? Please also provide the exact line from the document you used to answer the question\"\n",
    "index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Can I ask a question and get an answer back as a pydantic object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
